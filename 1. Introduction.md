# Introduction

This document summarizes the software I developed as my final year project.

I was asked to design a very fast *HTTP* traffic generator. The purpose of
this traffic generator was to simulate a very large amount of *HTTP* sessions
to qualify layer-4 middleboxes for deployment on production networks. The goal
was not to detect any error caused by the middlebox, but only to generate a 
sufficient *HTTP* traffic in order to check the ability of the middlebox to
endure a such throughput.

Desire was expressed that *HTTP* traffic could be generated at a rate of up to
40 Gbps, using four 10 Gbps *Ethernet* links.

It was required that the software should run on a *Tilera TILE-Gx36* device,
an extension card orchestrated by a proprietary *TILE-Gx* 36-core
low-consumption microprocessor. The device is designed to run high performance
network applications and has four 10 Gbps network interfaces

The project resulted in the design and development of *Rusty*, a light-weight,
event-driven and highly-scalable *TCP/IP* network stack.

Although network applications powered by *Rusty* are not able reach the 40 Gbps
goal, they benefit from an huge increase in performances when compared to the
*Linux* network stack running on the same hardware.

The high-performance web-server developed to generate *HTTP* traffic gets a
2.6× improvement in throughput when running on *Rusty* instead of the new
high-performance reusable *TCP* sockets introduced in *Linux 3.9*.

The web-server was able to deliver static *HTML* pages at a rate of 12 Gbps
*TCP* on the *Tilera TILE-Gx36* device. This number could be significantly
improved by carrying out some optimizations that have not been implemented,
because of a lack of time.

The network stack directly calls the proprietary *Tilera*'s network driver,
by-passing the operating system. It has been designed to be easily adapted to
other user-space network drivers, available with some professional NICs. As
performances are currently bounded by the relatively low processing power of the
*TILE-Gx* microprocessor, an huge increase in performances could also be
expected by porting *Rusty* to a faster CPU architecture.

Source code of the new *TCP/IP* stack and of the web-server is available at
[github.com/RaphaelJ/rusty/](https://github.com/RaphaelJ/rusty/).

## A very fast *HTTP* traffic generator

The goal of this final year project was to develop a test-bed to evaluate the
capacity of network middleboxes currently developed at the *University of
Liège* to sustain high loads of *HTTP* traffic.

The middleboxes are to be located between one (or several) *HTTP* request
generator(s) and one (or several) *HTTP* server(s). *HTTP* request gerator(s)
and *HTTP* server(s) should generate as much *HTTP* sessions as possible, and 
should be able to tell at which rate traffic was delivered.

The ultimate goal of the project was to make up to 40 Gbps of *HTTP* traffic
going through the middlebox.

![Middlebox Testbed](img/middlebox_test.png)

Middleboxes are expected to have four 10 Gbps *Ethernet* links. To benefit from
the full-duplex capacity of the links, it was strongly suggested that *HTTP*
requests and responses should transit in both directions of a single link.
Suppose that you have two full-duplex links between the middlebox and a single 
testing device (as in the following diagram), the request generator running on 
the first link could send requests to the web-server running on the second link, 
and vice-versa. This doubles the traffic passing through the middlebox per link.

![Middlebox Testbed same device](img/middlebox_test_same_device.png)

The university network laboratory has a *Tilera TILE-Gx36* device with four
10 Gbps *Ethernet* links. It was requested that the entire test-bed should run
on this device. The device use a proprietary 36-core CPU, runs the *Linux* 
operating system, and has network co-processors for packet classification.

It appeared at a very early stage in the project that the *Linux* network
stack will never be able to sustain the requested amount of traffic on requested
device. To address this issue, a new high-performance *TCP/IP* stack was
developed from scratch. This new stack, named *Rusty*, significantly improved
the performances, but I was not able to reach the targeted 40 Gbps throughput.

*Rusty* is not yet able to initiate connections, because of an
unresolved issue with the *Tilera*'s driver when using multiple cores. The
issue is described in the *Improvements* chapter. Because of that, the *HTTP*
request generator is running on another device, an high-end *x86* server with
several 10 Gbps *Ethernet* NICs. This server is fast enough to sufficiently
load the web-server running on the *TILE-Gx36*.

The web-server running on the *TILE-Gx36* has been written specifically
for this project. The request generator running on the *x86* is *wrk*,
a scriptable *HTTP* benchmarking tool [Gloz15].

![Middlebox Testbed tilera frodo](img/middlebox_test_tilera_frodo.png)

### Simulated traffic

Middleboxes are developed to remove specific content in web pages, such as
advertisements.

In order to simulate this use case, the simulated traffic consists of random
requests for the front pages of the 500 most popular websites (according to
*Alexa* [Alex15]). The pages have been downloaded locally. Page sizes range from
3.5 KB ([wellsfargo.com](http://wellsfargo.com)) to 1.1 MB
([xcar.com.cn](http://xcar.com.cn)). 70% weight less that 150 KB, 90 % weight
less than 300 KB. Only 17 (3 %) weight more that 500 KB. The following graph
shows the distribution of page sizes in the 500 pages set:

![Page size distribution](img/page_sizes.png)

TCP connection are not used for more than one HTTP request. A new TCP handshake
is initiated before each HTTP request.

## Rusty

*Rusty* is the name of the *TCP/IP* stack that was developed to increase the
throughput of the traffic simulator.

It follows the original TCP specification [RFC793] with the congestion
control algorithms described in \[RFC5681\] (slow start, fast retransmit and
and fast recovery).

It has a very unique architecture, designed for efficiency and high-scalability:

![Single core architecture](img/architecture_single_thread.png)

* It is a **bare-metal** network stack. Apart from its initialization and some
  memory allocations, it does not rely on the operating system. It uses the
  network driver while in user-space and does not produce any system call
  (except for  some rare cases such as page fault handling). Once initialized,
  it binds itself to some CPU cores and disables preemptive multi-tasking for
  these cores (so  the operating system do not issue context-switches). This is
  detailed in the *Implementation details* chapter.
* It is an **highly-parallelizable** network stack. It spawns as much instances
  of itself as the number of CPU cores it can use. Each instance handles only a
  subset of the TCP connections — a given connection is always handled by the
  same core —, and does not share anything with other stacks.
  There is no mutual exclusion mechanism and the software scales almost linearly
  with respect to the number of processing cores (*Rusty* is 31 times faster on
  35 cores than on a single core !). The architecture is presented in the
  *Implementation details* chapter while the observed scalability is discussed
  in *Performance analysis*.

* It is an **event-driven** network stack. The application layer responds to
  events, such as a new connection, a new arrival of data, or a notification
  that the remote user closed the connection. The application layer does this
  by providing function pointers that handle these events. As the *Rusty*
  instance that handle the connection and the application both run on the same
  core and thread, in user-space, the overhead of passing data from the network
  stack to the application layer is only that of a function pointer call.
  It is radically different from traditional stacks where blocking calls such as
  `recv()` or `accept()` produce system calls, and often context-switches.
* It has a **zero-copy** interface. The application layer directly accesses the
  memory buffers used by the network interface. `send()` and `recv()` calls, as
  implemented in most network stacks, require the data to be copied from and t
  application buffers. The machinery behind this zero-copy interface is detailed
  in the *Implementation details* chapter.


> **Note** *Rusty* also distances itself from the original TCP specification
> by ignoring *Urgent* and *Push* flags. They do not make much sense in an
> event-driven *TCP/IP* stack, as segment payloads are always delivered to the
> application layer at the instant they are received.

To improve performances, *Rusty* also provides a way to **pre-compute *Internet*
checksums** of transmitted data. This gives a noticeable speedup for
applications, such as the web-server, where the data that can be transmitted is
determined in advance. Pre-computing checksums is not as easy as it may look,
because you do not know in advance how the data will be chunked in *TCP*
segments. The technique used is described in the *Implementation details* 
chapter.

## Performance overview

This section only gives summarizes the performances and the scalability of
*Rusty*. Detailed analysis and methodology is available in the *Performance
analysis* chapter.

*Rusty* shows an exceptional ability to scale up with additional processing
cores. This exhibits the efficiency of the multi-threading model that has been
put in place.

The following graph compares the maximum throughput of the same *HTTP* server
on the *Tilera TILE-Gx36* with a given number of CPU cores, when using *Rusty*
and when using the *Linux* stack. The *Linux* version uses *epoll* with reusable
sockets, which is the advised method to write efficient network applications
by the *Linux* kernel developers [Kerr13].

![Performances and scalability](img/scalability.png)

With all the 36 cores are enabled, the *Rusty* implementation puts 12 Gbps on
the wire (about 11,000 *HTTP* requests second). This is a 2.6× improvement
against the *Linux* version which is only able to deliver 4.6 Gbps (about 4,000
*HTTP* requests second).

While these performances are engaging, the *TCP/IP* stack has not yet been
seriously optimised (because of a shortage of time). Dynamic memory
allocation (currently required to handle timers, transmission queues and
functionnal closures) consumes a large fraction of the CPU cycles required to
handle a segment and could be widely improved.

^ NOTE ^ AJOUTER LE TEMPS DES MALLOCS

Performances are also relatively low because of the relative inneficiency 
of the *TILE-Gx36* processor. Performances are mostly bounded by the computing
power of the CPU, and this chip is about ten times slower than modern
*x86* microprocessor.

^ NOTE ^ AJOUTER x86 vs TILE-Gx

## Similar network stacks

*Rusty* is not the first user-space *TCP/IP* stack designed for high
performances.

I mostly looked at two existing stacks before and while developing *Rusty*:
*mTCP* [JWJ⁺14] and *Seastar* [Clou15]. Both use an user-space driver for some
*Intel* 10 Gbps *Ethernet* NICs. *Seastar* has a similar architecture to
*Rusty* while *mTCP*'s architecture is quite different.

### mTCP

![mTCP architecture](img/mtcp_architecture.png)

*mTCP* provides an implementation of *epoll* and *BSD* sockets that runs
in user-space (and is thus almost backward compatible with simple
applications running on these).

Unlike *Rusty* (and *Seastar*), the network stack does not run on the same
threads as the application layer. Each application layer thread is coupled with
an *mTCP* thread (that runs the network stack and the network driver in
user-space). Both are affinitized to the same CPU core. They share a event
queue and a job queue. Events (such as the reception of new data) are produced
by the *mTCP* thread and consumed by the application layer thread, while the
jobs (such as new data to transmit) are produced by the application thread and
executed by the *mTCP* thread.

Connections are local to a single *mTCP* thread (they are not shared). Because
of that, they achieve a better scalability than the *Linux* *TCP/IP* stack.
They chose to run the application layer on a separate thread so it does not
affect the behaviour of *TCP* (a long computation in the application layer on
*Rusty* can produce retransmissions from the remote as the processing of 
incoming packets would be delayed). The drawback is that they must carry the
overhead of the operating system scheduler, and that they can not provide a
zero-copy interface as *Rusty* does.

On a 8-core (16 hardware threads) *Intel Xeon*, they achieve a 2.2× improvement
in performances compared to the *Linux* stack with reusable sockets on small
(64 B) messages. They also point out that the *Linux* stack is not able to
scale correctly over more than 4 cores on this hardware.

In the first days of my Master's thesis, I was suggested to modify this stack so
it can be used on the *TILE-Gx36*. But, after going through the source code,
I thought that it would be way more painful to modify this poorly documented
software than creating a new *TCP/IP* stack myself. Also, the software
is strongly coupled to the network driver it uses, making the porting work even 
harder. *mTCP* was developed more as a proof-of-concept than as a reusable 
system.

### Seastar

*Seastar* is an open-source framework developed by *Cloudius Systems*.
It was released to the public for the first time during the second half of
February 2015, and I became aware of it by the end of April (at a time when
*Rusty* was already seriously being developed). The project is still actively
developed, with 10 different contributors during the month of July 2015,
according to the the public code repository.

![Seastar architecture](img/seastar_architecture.png)

The framework is slightly more advanced than *Rusty* or *mTCP*. Like *Rusty*,
it runs a single thread per core. However, it allows the application layer
to run multiple cooperative tasks, or coroutines: in addition to its *TCP/IP*
stack, the *Seastar* runtime provides a cooperative multitasking system.
The application layer executes itself in these coroutines, and must give
execution control back to the runtime voluntarily, or by making call to 
`send()`, `recv()` or other function calls that would be blocking in traditional
network stacks. Unlike operating system threads, coroutines are never preempted.

This cooperative model has a small overhead, as coroutines have their own
call stacks. Context-switches are however significantly faster than those of an
operating system (such as in *mTCP*), as it is only about substituting CPU
registers. The scheduling algorithm is also simpler, and mostly relies on events
from the network.

Unlike *mTCP*, *Seastar* is not backward compatible with software written using
*BSD* sockets. Applications must be written with the cooperative multitasking
model in mind.

> **Note**
> A such cooperative multitasking system could be implemented on top of *Rusty*.
> The event-driven model is a kind of lower level interface with the network
> stack.

The *Seastar* framework could have been modified to run on the *Tilera*'s 
network driver, as the network stack is reasonably disassociated from the 
network driver. It would still be an huge amount of work (network buffers work 
very differently in the driver they use (*DPDK*), and the framework has become 
quite huge), but it would be definitively doable within the context of this 
Master's thesis. I did not do that because I was already too advanced in my own 
*TCP/IP* stack at the time I became aware of the *Seastar* project.
