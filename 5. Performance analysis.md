# Performance analysis

In this chapter, we will see how the performances of a minimal web-server
improve when it uses *Rusty* instead of the *Linux* network stack on the 
*TILE-Gx36* device.

## Simulated traffic and test conditions

The *Rusty* framework has been developed to create an application able to
generate a large amount of *HTTP* sessions in order to qualify middleboxes for
highly loaded production networks. These middleboxes are developed to remove
specific content in web pages, such as advertisements.

In order to simulate this use case, the simulated traffic consists of random
requests for the front pages of the 500 most popular websites (according to
*Alexa* [Alex15]). The pages have been downloaded locally. Page sizes range from
3.5 KB ([wellsfargo.com](http://wellsfargo.com)) to 1.1 MB
([xcar.com.cn](http://xcar.com.cn)). 70% weight less that 150 KB, 90 % weight
less than 300 KB. Only 17 (3 %) weight more that 500 KB. The following graph
shows the distribution of page sizes in the 500 pages set:

![Page size distribution](img/page_sizes.png)

*TCP* connections are not used for more than one *HTTP* request. A new *TCP*
handshake is initiated for every *HTTP* request. During the following tests,
web-servers have been flooded with 1000 concurrents *TCP* connections during 50 
seconds, using *wrk*, a scriptable *HTTP* benchmarking tool [Gloz15].

Please remember that *HTTP* requests are currently not generated on the
*TILE-Gx36*, but on a separate *x86* server. This server fully capable of
generating the amount of requests required for the tests to follow. The
*TILE-Gx36* only serves the pages via a web-server.

## Minimal web-server

To minimize the overhead brought by full featured web servers, I wrote a
minimal web-server that runs on top of *Rusty* or on top of *BSD* sockets as
provided by the *Linux* operating system. This server only reads the first line
of the *HTTP* requests, and ignores any other *HTTP* header. Because of this, it
is not fully compliant with the *HTTP* protocol, but still adequate to simulate
web traffic.

This minimal web-server also preloads in memory all the pages that can be
served. It was observed that this slightly increases performances, even 
compared to a `sendfile()` system-call.

> **Note** `sendfile()` is used to transfer data between two file descriptors.
> Because this copying is done within the kernel, it does not require
> transferring data from user-space. File descriptors also identify sockets
> in UNIX operating systems.

## Shared BSD sockets

When running on top of the *Linux* stack, the minimal web-server uses the
*epoll* system-calls to watch for events on the *TCP* connections it manages. 
It also uses the *shared sockets* feature which has been introduced in *Linux 
3.9* (April 2013). 

The web-server spwans a defined number of *worker threads*. Each worker thread
has its own *epoll*. An *epoll* is an event queue shared by the operating system
and the application. It allows the kernel to notify the application of events
on a set of file descriptors (events like a new connection or a new arrival of
data). Workers are constantly waiting for events on their queues, and processes
them as they arrive. All workers share a single server/listen socket (using the
new `SO_REUSEPORT` feature introduced in *Linux 3.9*). On new connections on
this shared socket, only one worker is notified, and receives the new client's
socket. The worker adds this socket to its event poll and processes any
event on it. The number of workers is usually lower or equal to the number of
cores on the hardware that runs the software. Without shared sockets, another
worker should be dedicated to listening for events on the server socket and to
load balance any new connection on the other workers.

![Shared sockets and epoll](img/linux_reuse_epoll.png)

This architecture is more efficient than the traditional "spawn a new thread
for each new incoming connection". It is the advised way of writing efficient
network applications on top of the *Linux* kernel [Kerr13].

The following graph shows, in terms of *HTTP* throughput, how this minimal
web-server (*Mininal WS* in the graphs) compares against full-featured servers
on the *TILE-Gx36*. *Lighttpd* (version 1.4.28) is a widely used *HTTP* server 
which uses *epoll* (but not shared sockets) while *lwan* is a lightweight server 
that uses *epoll* and shared sockets [Pere15].

> **Note**
> *nginx* (which is build around shared sockets and *epoll*) was also intended
> to be featured in this comparison, but I did not succeed at compiling it on
> the *Tilera* architecture. On the *x86* server, *lwan* is significantly
> faster than *nginx*.

![Comparison of server throughput](img/comparison_servers_throughput.png)

The following graph shows how much CPU resources are spent in the kernel and in
user-space during the processing of an *HTTP* request:

![Comparison of server CPU time](img/comparison_servers_cpu_time.png)

The minimal web-server only spends 6% of its time in user-space. The only
system-calls it does are related to the sockets it handles, as files are
preloaded in memory during the initialisation. I think it is fair to say that
its performances are very close to the best that one can expect to get from an
network application running on top of the *Linux* stack.

## Comparing Rusty and the Linux stack

The following graph shows how the minimal web-server performances increase with
the number of enabled cores on the *TILE-Gx36*:

![Performances and scalability](img/scalability.png)

*Rusty* performances scales linearly with the number of cores. It fulfills a
single 10 Gbps *Ethernet* link with 29 cores. The throughput with a single
worker tops at 395 Mbps, while it reaches 12 Gbps with 35 workers.

> **Note** The *Rusty* throughput on one core is zero because the framework
> requires at least two cores to operate: one that executes the network stack,
> and another for the operating system.

The *Linux* stack, unlike *Rusty*, fails to scale correctly after about seven
cores. The problem is a well-known fact and is caused by a thread contention 
when allocating and dereferencing file descriptors [Boy‚Å∫08].

## Changing the MTU

## Cost centres

## TILE-Gx vs x86
