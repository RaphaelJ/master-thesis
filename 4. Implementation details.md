# Implementation details

The first section summarizes the specifics of the used 36-cores *Tile-Gx*
architecture.

These second section 

## Development environment summary

The framework have been developed for the ***Tile-Gx36* processor**.

The *Tile-Gx* processor family has been developed by *Tilera*, until the company
was acquired by *EzChip* during the summer 2014.

The family has been declined in 9, 16, 36 and 72-cores models. These use a
proprietary 64 bits RISC instruction set and run at a **1.2 Ghz frequency**.

*Tile-Gx* processors have two *DDR3* memory controllers, and a theoretical
**maximal memory bandwidth of 25.6 Gbps**.

These CPUs are designed to run network related software. As a result, they
also have:

* *Co-prosessors* for packet analysis, network flow load-balancing, compression
  and encryption.
* *Integrated 10 Gbps Ethernet controllers* to maximise network throughput,
  especially on small packets.

The device used to implement this project is engineered around a 36-core
*Tile-Gx* chip, in the form of a *PCI-Express* extension card. It is a computer
on its own, having its dedicated *DDR3* memory and **four 10 Gbps *Ethernet*
links**. It runs the *Linux 3.10* operating system.

## Parallelisation and scalability

![General architecture](img/architecture.png)

**The framework has been designed to be highly parallelizable**, benefiting from
the *hardware network load-balancer* included in the *Tile-Gx36* processor.

**The hardware load-balancer has the ability to distribute incoming network
packets around multiple software queues**. Queue selection is based on a
combined hash function of the *IPv4* addresses and *TCP* ports of the incoming
packet.
Namely, packets belonging to a same *TCP* flow (source address, source port,
destination address and destination port) are always delivered to the same
software queue.

When the framework runs on several CPU cores, as much *TCP/IP* stacks and
ingress queues are instancied. **Each network stack instance is continuously
polling its queue for a new incomming packet to process. A core is entirely 
dedicated to the execution of its associated network stack** (the operating
system can not initiate a context-switch as preemptive multi-tasking is disabled
for these cores). The main loop executed by each core looks like this one:

```C
while (app_is_running) {
    execute_expired_timers();

    if (incomming_queue.is_empty())
        continue;

    packet_t packet = incomming_queue.pop();
    network_stack.receive_packet(packet);
}
```

> **Note**
> One can notice that the timers set is continuously checked for expired timers.
> Timers are detailed later in this document.

The network driver, provided by the *Tilera development framework*, runs in
user space and does not require any system call when transmitting or
receiving packets.

> **Note**
> Processes can still enter kernel space, because of a system call, some
> interruption, or a page fault.

Each running core will be in charge of dealing with its own subset of *TCP*
connections (as determined by the load-balancer), and will not share any
connection with another core. **Cores do not share any mutable state**, no
mutual exclusion mechanism is required.

> **Note**
> One could argue that the *ARP* layer should share a common table between
> network stacks.
>
> As of today, when running on multiple cores, *ARP* layers share a static
> table, and any request to an unknown *IPv4* address fails.
> Dynamic ARP tables (i.e. on which new entries can be added when receiving
> *ARP* replies) are implemented when running on a single core.

**Event handlers composing the application layer that are related to a same TCP
connection will always be executed on the same core**. The application layer
directly read from and write to memory buffers used by the network interface.

### Advantages

* **It makes the framework implementation simpler**, as there is no interactions
  between concurrently executing tasks. All things considered, the best way of
  dealing with concurrent programming problems, is probably to avoid
  concurrency.
* **It makes the framework very scalable**. As there is no mutual exclusion,
  adding more processing cores should increase performances until the maximal
  throughput of the memory or of the network hardware is reached.
* **The overhead of context-switching, inter-process data sharing and system
  calls is removed**, as application layer event handlers are executed on the
  core that processes the connection. The framework is also more CPU cache
  friendly, as temporal locality is high.

### Drawbacks

* **The throughput of a single TCP connection is upper bounded by the processing
  power of the core that is handling it**. There is no per-connection
  parallelism. While the framework is able to sustain a several Gbps rate over
  a large number of connections, the maximum rate which can be reached on a
  single connection peaks at around 350 Mbps (*TileGx* cores are relatively
  slow). Similarly, if the load-balancer fails to evenly distribute packets
  over receiving queues, or if some connections are more heavy that others,
  some cores could be overcharged while some others could be partly idle.
* **Programming without preemptive multi-processing could be confusing, and some
  applications could be harder to reason about**.

## Cache hierarchy and memory locality

Each *TileGx* core has a 32 KB L1 instruction cache, a 32 KB L1 data cache and a
256 KB unified L2 cache. There is no shared L3 cache but **L2 caches can be
accessed from others cores** with an added latency. The following table
summarizes memory subsystems of and their main properties [Tile12].

| Memory            | Size               | Associativity | Latency (CPU cycles)|
| ----------------- | ------------------ | ------------- | --------------------|
| L1 instruction    | 32 KB              | 2 way         | 2                   |
| L1 data           | 32 KB              | 2 way         | 2                   |
| Local L2          | 256 KB             | 8 way         | 11                  |
| Other core L2     | 36 x 256 KB = 9 MB | 8 way         | 32 to 41            |
| RAM (*DDR3-1600*) | 16 GB        | Irrelevant | 67 if page in TLB, 88 if not |

The *TileGx* architecture also allows software developers to choose where memory
pages are cached in the shared L2 cache. By default pages are cached over the
whole set of cores (*hash-homed pages*), the least significant bits of memory
addresses determining on which core cache lines are located. **The alternative
is to link a page to a specific core (*homed pages*), in which case memory
entries of this page will all be cached in this core**. Ideally, data shared
among several cores should be *hash-homed* while memory accessed by a single
core should be *homed*. It is still possible to access *homed* memory cached on
another core, but the latency is about three to four times higher.

In order to take advantage of this cache hierarchy, **data related to each TCP
connection** (connection state, transmission windows, transmission queues ...)
**are cached on the core that handle the connection**. 

## Abstracting the network stack

One of the main goals while designing the framework was to make the network
chunk of the software as loosely coupled as possible from the network driver,
while still being as efficient as possible.

**The outcome is that the framework can be relatively easily ported to another
network driver**, without the Ethernet, *IPv4*, *ARP* or *TCP* pieces of code
requiring any change.

The *Ethernet* layer accepts a "driver type" as a *C++* template argument. The
driver type shall provide some functions, such as one to send packets, and must
transfer to the Ethernet layer any received packet. **The driver must also
provide a way to manage network buffers**, as described in the following
section.

Others layers of the network stack follow the same design. The *IPv4* layer
accepts a underlying "data-link type", the *ARP* layer a "data-link type" and a
"network type" types, and *TCP* accepts a network type. **One could write a new
*IPv6* type and be able to run the existing *ARP* and *TCP* layers as they exist
today**.

The *templates* and *member types* features of the *C++* programming languages
were heavily used to put together this kind of genericity, without having any
impact on performances.

## Interacting with network buffers

**The framework interacts with the driver by receiving and by passing
*Ethernet* frames in *network buffers*.** Network buffers contains the raw 
bytes that the network interface received or is expected to transmit.

**In the case of the *Tilera development framework* network driver, network
buffers allocation is managed by the network hardware** and they reside on a
previously allocated memory page that the network controller is aware of.
**Multiple buffers can be chained together**. **A buffer chain only contains a
single frame**. The following figure shows three chained network buffers.

![Network buffers](img/network_buffers.png)

Every buffer, except the last one, contain a *descriptor* of the next buffer in
the chain (the descriptor tells memory address and the size of the buffer, and
if it is the last buffer in the chain).

Some others drivers (such as the one provided by *Intel* for its professional
NICs) also support some kind of chaining.

### Higher level interface

The way buffers are represented is specific to each hardware and driver. **To
keep the network stack loosely coupled to the driver, it must interact with
network buffers through an higher level interface**. 

The application event handlers also access transmitted data through this
interface.

The goals while designing this interface were:

* To not produce any (noticiable) performance overhead.
* To provide a way to directly work on the buffer's bytes when possible, without
  copying data.
* To be easy and simple to use.
* To have slicing operators. Slicing makes it passing sub-sections of a packet
  to the upper network layer easier.

The framework use an data structure named ***cursors*** to abstract network
buffers. Cursor is an identical data structure taht can be used to read and
write data. It is highly influenced by *ByteStrings*, a very simple yet powerful
I/O abstraction designed by *Don Stewart* for the *Haskell* programming
language [Stew05]. It shares some usage patterns with *C pointers* and
*iterators* of the *C++ STL*.

A cursor is an abstract data-type representing a position in a byte-stream,
with a way for the user to know how many bytes remain from this position, and
ways to move the cursor:

* `size_t size()` is a method on a cursor that returns the number of bytes that
  can be read before reaching the end of the stream.
* `cursor_t drop(size_t n)` is another method that returns a new cursor
  pointing `n` bytes after the cursor on which it is applied.
* `cursor_t take(size_t n)` returns a new cursor containing only the first `n` 
  bytes of the cursor on which the method is applied.

> **Note**
> `drop` and `take` can be used together to create sub-sections (*slices*) of
> a byte-stream. 

All methods return a new cursor, without modifying the cursor on which they
are applied. This makes cursor usage clearer (cursors can be used in
expressions without any side-effect), and makes implementing backtracking
effortless.

To read and write data from and into the buffer, two additional methods must be
provided by the cursor. `cursor_t read(char *data, size_t n)` and
`cursor_t write(const char *data, size_t n)` can be used to copy `n` bytes of
data into/from the given buffer. Again, both methods return a new cursor,
pointing to the first byte after the read/written data.

### Zero-copy read and write

The `read` and `write` methods presented are pretty straightforward but do not
provide a zero copy interface. Indeed, data is copied to/from the given memory
buffer at each call.

One could write a `read`-like method that returns a pointer to the data directly
in the network buffer, but it will not work when the requested data is spread
over multiple chained buffers.

A solution could be to copy the data that overlaps over multiple buffers in
another temporary memory when it happens. There would be zero memory copy
except in the rare case of a such overlap.

The usage could look like:

```C++
// Reserves some temporary memory required if the data needs to be copied
// because of being spread over several buffers.
ipv4_header_t hdr_content;

// The method will either return a pointer to the data directly in the network
// buffer, or a pointer to 'hdr_content' with the data being copied there.
const ipv4_header_t* hdr = cursor.read(sizeof (ipv4_header_t), &hdr_content);

// [Processes the IPv4 header at 'hdr'].
```

Applying this same idea on a `write`-like function would be more complex, as the
data would need to be copied back into the buffer after the temporary memory was
used:


```C++
// Reserves some temporary memory.
ipv4_header_t hdr_content;

// The method will either return a pointer to the data directly in the network
// buffer, or a pointer to 'hdr_content'.
ipv4_header_t* hdr = cursor.start_write(sizeof (ipv4_header_t), &hdr_content);

// [Writes something into the IPv4 header at 'hdr'].

// This call will do nothing if 'start_write' gave us pointer to the network
// buffer. Otherwise, it will copy the content of 'hdr_content' into the
// network buffer.
cursor.end_write(sizeof (ipv4_header_t), hdr);
```

Instead of offer this not really convenient pair of `write_` methods, the
framework provides an higher-order method (i.e. a method that accepts a 
function as argument) that catches this pattern:

```C++
cursor.write_with(
    sizeof (ipv4_header_t),
    [] (ipv4_header_t* hdr)
    {
        // [Writes something into the IPv4 header at 'hdr'].
    }
);
```

The method will either provide to the given function a pointer to the network
buffer, or to a temporary memory if the data is spread over several network
buffers. A similar `read_with` method exists to read data without copying.

> **Note**
> The `write_with` and `read_with` methods are inlined by the compiler. This
> usually removes the overhead of using a closure.

### Cursors implementation

Implementing cursors is very straightforward. The following diagram shows how
cursors are implemented in the case of *Tilera*'s chained buffers:

![Buffer cursor](img/buffer_cursor.png)

The first field is a pointer to the next byte to read/write in the current 
buffer. The last field, telling how many bytes is remaining in the whole buffer
chain (here pictured by two chained buffers), is required to implement slicing
efficiently. `take` is  implemented as an *O(1)* operation.

### Writers

## Checksum

### Pre-computed checksums

## Timers

## Writers

## Micro optimisations

### Prefetching

NOTE: UTILE ??

To avoid cache misses, prefetching was used in some places, such as:

* **Upon receiving a packet**. The whole packet is prefetched before starting
  any processing. As packets are relatively small (1,500 bytes, or 9 KB if
  *Jumbo  Frames* are enabled), they should fit in the 32 KB L1 cache.
* **Upon transmitting a packet**. The data to copy into the payload is
  prefetched before being written.


## Performance analysis and cost centres


