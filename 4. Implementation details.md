# Implementation details

The first section summarizes the specifics of the used 36-cores *Tile-Gx*
architecture.

These second section 

## Development environment summary

The framework have been developed for the ***Tile-Gx36* processor**.

The *Tile-Gx* processor family has been developed by *Tilera*, until the company
was acquired by *EzChip* during the summer 2014.

The family has been declined in 9, 16, 36 and 72-cores models. These use a
proprietary 64 bits RISC instruction set and run at a **1.2 Ghz frequency**.

*Tile-Gx* processors have two DDR3 memory controllers, and a theoretical
**maximal memory bandwidth of 25.6 Gbps**.

These CPUs are designed to run network related software. As a result, they
also have:

* *Co-prosessors* for packet analysis, network flow load-balancing, compression
  and encryption.
* *Integrated 10 Gbps Ethernet chipsets* to maximise efficiency, especially on
  small packets.

The device used to implement this project is engineered around a 36-core
*Tile-Gx* chip, in the form of a PCI-Express extension card. It is a computer
on its own, having its dedicated DDR3 memory and **four 10 Gbps Ethernet
links**. It runs the *Linux 3.10* operating system.

## Parallelisation and scalability

![General architecture](img/architecture.png)

**The framework has been designed to be highly parallelizable**, benefiting from
the *hardware network load-balancer* included in the *Tile-Gx36* processor.

**The hardware load-balancer has the ability to distribute incoming network
packets around multiple software queues**. Queue selection is based on a
combined hash function of the IPv4 addresses and TCP ports of the incoming
packet.
Namely, packets belonging to a same TCP flow (source address, source port,
destination address and destination port) are always delivered to the same
software queue.

When the framework runs on several CPU cores, as much TCP/IP stacks and
ingress queues are instancied. **Each network stack instance is continuously
polling its queue for a new incomming packet to process. A core is entirely 
dedicated to the execution of its associated network stack** (the operating
system can not initiate a context-switch as preemptive multi-tasking is disabled
for these cores). The main loop executed by each core looks like this one:

```C
while (app_is_running) {
    execute_expired_timers();

    if (incomming_queue.is_empty())
        continue;

    packet_t packet = incomming_queue.pop();
    network_stack.receive_packet(packet);
}
```

> **Note**
> One can notice that the timers set is continuously checked for expired timers.
> Timers are detailed later in this document.

The network driver, provided by the *Tilera development framework*, runs in
user space and does not require any system call when transmitting or
receiving packets.

> **Note**
> Processes can still enter kernel space, because of a system call, some
> interruption, or a page fault.

Each running core will be in charge of dealing with its own subset of TCP
connections (as determined by the load-balancer), and will not share any
connection with another core. **Cores do not share any mutable state**, no
mutual exclusion mechanism is required.

> **Note**
> One could arge that the ARP layer should share a common table between network
> stacks.
>
> As of today, when running on multiple cores, ARP layers share a static table,
and any request to an unknown IPv4 address fails.
> Dynamic ARP tables (i.e. on which new entries can be added when receiving ARP
> ARP replies) are implemented when running on a single core.

**Event handlers composing the application layer that are related to a same TCP
connection will always be executed on the same core**.

### Advantages

* **It makes the framework implementation simpler**, as there is no interactions
  between concurrently executing tasks. All things considered, the best way of
  dealing with concurrent programming problems, is probably to avoid
  concurrency.
* **It makes the framework very scalable**. As there is no mutual exclusion,
  adding more processing cores should increase performances until the maximal
  throughput of the memory or of the network hardware is reached.
* **The overhead of context-switching, inter-process data sharing and system
  calls is removed**, as application layer event handlers are executed on the
  core that processes the connection. The framework is also more CPU cache
  friendly, as temporal locality is high.

### Drawbacks

* **The throughput of a single TCP connection is upper bounded by the processing
  power of the core that is handling it**. There is no per-connection
  parallelism. While the framework is able to sustain a several Gbps rate over
  a large number of connections, the maximum rate which can be reached on a
  single connection peaks at around 350 Mbps (*TileGx* cores are relatively
  slow). Similarly, if the load-balancer fails to evenly distribute packets
  over receiving queues, or if some connections are more heavy that others,
  some cores could be overcharged while some others could be partly idle.
* **Programming without preemptive multi-processing could be confusing, and some
  applications could be harder to reason about**.

## Cache hierarchy and memory locality

Each *TileGx* core has a 32 KB L1 instruction cache, a 32 KB L1 data cache and a
256 KB unified L2 cache. There is no shared L3 cache but **L2 caches can be
accessed from others cores** with an added latency. The following table
summarizes memory subsystems and their main properties [Tile12].

| Memory          | Size               | Associativity | Latency (CPU cycles) |
| --------------- | ------------------ | ------------- | -------------------- |
| L1 instruction  | 32 KB              | 2 way         | 2                    |
| L1 data         | 32 KB              | 2 way         | 2                    |
| Local L2        | 256 KB             | 8 way         | 11                   |
| Other core L2   | 36 x 256 KB = 9 MB | 8 way         | 32 to 41             |
| RAM (DDR3-1600) | 16 GB      | Irrelevant    | 67 if page in TLB, 88 if not |

The *TileGx* architecture also allows software developers to choose where memory
pages are cached in the shared L2 cache. By default pages are cached over the
whole set of cores (*hash-homed pages*), the least significant bits of memory
addresses determining on which core cache lines are located. **The alternative 
is to link a page to a specific core (*homed pages*), in which case the memory
entries from this page will all be cached in this core.** Ideally, data shared
among several cores should be *hash-homed* while memory accessed by a single
core should be *homed*. It is still possible to access *homed* memory cached on
another core, but the latency is about three to four times higher.

In order to take advantage of this cache hierarchy, **data related to each TCP
connection (connection state, transmission windows, transmission queues ...) are
cached on the core that handle the connection**. 

### Prefetching

NOTE: UTILE ??

To avoid cache misses, prefetching was used in some places, such as:

* **Upon receiving a packet**. The whole packet is prefetched before starting
  any processing. As packets are relatively small (1,500 bytes, or 9 KB if
  *Jumbo  Frames* are enabled), they should fit in the 32 KB L1 cache.
* **Upon transmitting a packet**. The data to copy into the payload is
  prefetched before being written.

## Abstracting the network stack

## No-copy interface


## Checksum

### Pre-computed checksums

## Timers

## Micro optimisations



## Performance analysis and cost centres


