# Implementation details

The first section summarizes the specifics of the used 36-cores *Tile-Gx*
architecture.

These second section 

## Development environment summary

The framework have been developed for the ***Tile-Gx36* processor**.

The *Tile-Gx* processor family has been developed by *Tilera*, until the company
was acquired by *EzChip* during the summer 2014.

The family has been declined in 9, 16, 36 and 72-cores models. These use a
proprietary 64 bits RISC instruction set and run at a **1.2 Ghz frequency**.

*Tile-Gx* processors have two DDR3 memory controllers, and a theoretical
**maximal memory bandwidth of 25.6 Gbps**.

These CPUs are designed to run network related software. Consequently, they
also have:

* *Co-prosessors* for packet analysis, network flow load-balancing, compression
  and encryption.
* *Integrated 10 Gbps Ethernet chipsets* to maximise efficiency, especially on
  small packets.

The device, used to implement and carry out the execution of this project, is
engineered around a 36-core *Tile-Gx* chip, in the form of a PCI-Express
extension card. It is a computer on its own, having its dedicated DDR3 memory
and **four 10 Gbps Ethernet links**. It runs the *Linux 3.10* operating system.

## Parallelisation and scalability

**The framework has been designed to be highly parallelizable**, benefiting from
the *hardware network load-balancer* included in the *Tile-Gx36* processor.

**The hardware load-balancer has the ability to distribute incoming network
packets around multiple software queues**. Queue selection is based on a
combined hash function of the IPv4 addresses and TCP ports of the incoming
packet.
Namely, packets belonging to a same TCP flow (source address, source port,
destination address and destination port) are always delivered to the same
software queue.

When the framework runs on several CPU cores, as much TCP/IP stacks and
ingress queues are instancied. **Cores are entirely dedicated to the execution
of their related network stack**. The operating system can not initiate a
context-switch as preemptive multi-tasking is disabled for this task. Each
network stack instance is continuously polling its queue for a new packet to 
process.

The network driver, provided by the *Tilera development framework*, runs in
user space and does not produce any system call overhead when transmitting or
receiving packets.

> **Note**
> Processes can still enter kernel space, because of a system call, some
> interruption, or a page fault.

![Stack architecture](img/architecture.png)

Each running core will be in charge of dealing with its own subset of TCP
connections (as determined by the load-balancer), and will not share any
connection with another core. **Cores do not share any mutable state**, no
mutual exclusion mechanism is required.

> **Note**
> One could arge that the ARP layer should share a common table between network
> stacks.
> In the current implementation, when running on multiple cores, ARP layers use
> a static table, and any request to an unknown IPv4 address fails. Dynamic
> ARP tables are implemented when running on a single core.

## Memory hierarchy

Each *TileGx* core has a *32 KB L1 instruction cache*, a *32 KB L1 data cache* 
and a *256 KB unified L2 cache*. There is no common L3 cache but L2 *caches can
be accessed from others cores* with an added latency. The following table 
summarizes memory subsystems and their main properties *[Tile12]*.

| Memory          | Size               | Associativity | Latency (CPU cycles) |
| --------------- | ------------------ | ------------- | -------------------- |
| L1 instruction  | 32 KB              | 2 way         | 2                    |
| L1 data         | 32 KB              | 2 way         | 2                    |
| Local L2        | 256 KB             | 8 way         | 11                   |
| Other core L2   | 36 x 256 KB = 9 MB | 8 way         | 32 to 41             |
| RAM (DDR3-1600) | 16 GB      | Irrelevant    | 67 if page in TLB, 88 if not |

The *TileGx* architecture allows software developers to choose where memory
pages are cached in the shared L2 cache. Pages can be either cached in a
specific core (*homed pages*), or spread over the whole set of cores
(*hash-homed pages*). Least significant bits of memory addresses determine where 
cache lines are located when using hash-homed pages.

In an ideal world, data shared among several cores should be *hash-homed* while
memory accessed by a single core should be *homed*.

## Abstracting the network stack

## No-copy interface


## Checksum

### Pre-computed checksums

## Micro optimisations



## Performance analysis and cost centres


